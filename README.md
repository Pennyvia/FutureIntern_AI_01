# FutureIntern_AI_01
AI internship Task number 1 at Future intern for text generation with GPT-2 using a customized dataset

# Text Generation Model

This project demonstrates a text generation model using transformers. The model is capable of generating coherent and contextually relevant text based on a given prompt.

## Table of Contents
- Introduction
- Features
- Installation
- License

## Introduction
This project showcases a text generation model built using the Hugging Face Transformers library. The model can generate text based on various prompts, making it useful for applications such as story generation, dialogue systems, and more.

## Features
- Text generation using beam search, top-k sampling, and top-p sampling.
- Repetition penalty to reduce redundant outputs.
- Customizable parameters for text generation.

## Installation
To install the necessary dependencies, run the following command:
```bash
pip install transformers datasets

#Open it in google Colab and create specific folders like results and model 
